


import warnings
warnings.filterwarnings('ignore')





import seaborn as sns

DF = sns.load_dataset('iris')





DF.info()


DF.head(3)








DF.species.value_counts()





import matplotlib.pyplot as plt
import seaborn as sns

sns.pairplot(data = DF,
             hue = 'species')
plt.show()








X = DF[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]
y = DF['species']








from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size = 0.3,
                                                    random_state = 2045)

print('Train Data : ', X_train.shape, y_train.shape)
print('Test Data : ', X_test.shape, y_test.shape)











from sklearn.ensemble import RandomForestClassifier
RandomForestClassifier?


from sklearn.ensemble import RandomForestClassifier

Model_rf = RandomForestClassifier(n_estimators = 10,
                                  max_features = 2,
                                  random_state = 2045,
                                  n_jobs = -1) # CPU 전부다
Model_rf.fit(X_train, y_train)





y_hat = Model_rf.predict(X_test)








from sklearn.metrics import confusion_matrix, accuracy_score

print(confusion_matrix(y_test, y_hat))





print(accuracy_score(y_test, y_hat))








Model_rf.feature_importances_





plt.figure(figsize = (9, 6))
sns.barplot(x = Model_rf.feature_importances_,
            y = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])
plt.show()











from sklearn.ensemble import RandomForestClassifier

Model_rf = RandomForestClassifier()





params = {'n_estimators':[10, 50, 100, 150],
          'max_features':[2, 4],
          'max_depth':[2, 3, 4, 5, 6],
          
          'min_samples_split': [3, 4, 5, 6, 7], 
          # 노드를 분할하기위한 최소한의 샘플 데이터수, 과적합 제어에 사용,
          # 작게 사용할 수록 분할 노드가 많아져 과적합 가능성 증가
          
          'random_state':[2045]}





from sklearn.model_selection import GridSearchCV, KFold

grid_cv = GridSearchCV(Model_rf,
                       param_grid = params,
                       scoring = 'accuracy',
                       cv = KFold(n_splits = 5),
                       refit = True, # 최적 파라미터 모델 저장  model.best_estimator_
                       n_jobs = -1)





from datetime import datetime
start_time = datetime.now()

grid_cv.fit(X_train, y_train)

end_time = datetime.now()
print('Elapsed Time : ', end_time - start_time)








grid_cv.best_score_ 





grid_cv.best_params_








Model_CV = grid_cv.best_estimator_ 





y_hat = Model_CV.predict(X_test)





from sklearn.metrics import confusion_matrix, accuracy_score

print(confusion_matrix(y_test, y_hat))





print(accuracy_score(y_test, y_hat))





import warnings
warnings.filterwarnings('ignore')


import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, f1_score, accuracy_score

dataset = pd.read_csv('data/Social_Network_Ads.csv')


dataset.head()


X = dataset.iloc[:, [2,3]]
y = dataset.iloc[:, 4]


# dataset 을 Training 과 Test set 으로 분리
X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=0.2, random_state=0)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


# Feature Scaling
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test  = sc.transform(X_test)





# Training set 에 대해 Random Forest Classifier model 을 fitting
rf = RandomForestClassifier(n_estimators=10, 
                            criterion='entropy', random_state=0)
rf.fit(X_train, y_train)


y_pred = rf.predict(X_test)

print(y_pred)
print()
print("Test set true counts = ", sum(y_test))
print("predicted true counts = ", sum(y_pred))
print("accuracy = {:.2f}".format(sum(y_pred == y_test) / len(y_test)))


# making confusion matrix
print("confution matrix\n", 
      confusion_matrix(y_test, y_pred, labels=[1, 0]))
print()
print("f1 score\n", f1_score(y_test, y_pred, labels=[1, 0]))





from sklearn.ensemble import GradientBoostingClassifier

gb = GradientBoostingClassifier(learning_rate=0.1, 
                                n_estimators=500, max_depth=5)
gb.fit(X_train, y_train)


y_pred = gb.predict(X_test)

print(y_pred)
print()
print("Test set true counts = ", sum(y_test))
print("predicted true counts = ", sum(y_pred))
print("accuracy = {:.2f}".format(
            sum(y_pred == y_test) / len(y_test)))


# making confusion matrix
print("confution matrix\n", 
      confusion_matrix(y_test, y_pred, labels=[1, 0]))
print()
print("f1 score\n", f1_score(y_test, y_pred, labels=[1, 0]))





from matplotlib.colors import ListedColormap

# Create color maps
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])   
cmap_bold = ListedColormap(['#FF0000', '#00FF00'])    


x1_min, x1_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1       
x2_min, x2_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1  

X1, X2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1), 
                     np.arange(x2_min, x2_max, 0.1))
XX = np.column_stack([X1.ravel(), X2.ravel()])
Y_rf = np.array(rf.predict(XX))
Y_gb = np.array(gb.predict(XX))


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), sharey=True)

# Random Forest
ax1.pcolormesh(X1, X2, Y_rf.reshape(X1.shape), 
               cmap=cmap_light, shading='auto') 
for i in range(2):
    ax1.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], s=20, 
                color=cmap_bold(i), label=i, edgecolor='k')
ax1.set_title('Random Forest')
ax1.set_xlabel('Age')
ax1.set_ylabel('Estimated Salary')
ax1.legend()

# Gradient Boosting
ax2.pcolormesh(X1, X2, Y_gb.reshape(X1.shape), 
               cmap=cmap_light, shading='auto') 
for i in range(2):
    ax2.scatter(X_test[y_test == i, 0], X_test[y_test == i, 1], s=20, 
                color=cmap_bold(i), label=i, edgecolor='k')
ax2.set_title('Gradient Boosting')
ax2.set_xlabel('Age')
ax2.legend()
plt.tight_layout()


gb.feature_importances_       # Age, EstimatedSalary 의 중요도


feature_imp = pd.Series(gb.feature_importances_, 
            ['Age', 'EstimatedSalary']).sort_values(ascending=False)
feature_imp


feature_imp.plot(kind='bar', title='feature importance')



